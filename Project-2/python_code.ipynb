{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda06cf2",
   "metadata": {},
   "source": [
    "# Press ```Run All``` after changing the ```is_demo``` & ```is_training``` variables below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb20958",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_demo = False  # Set to True for demonstration, False for actual data collection and training\n",
    "is_training = True  # Set to True for training, False for data collection and demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ae5c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import serial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def seed_all(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format: Object-Distance-Height\n",
    "class_names = [\"Coin-10cm-10cm\", \"Coin-10cm-30cm\", \"Coin-30cm-10cm\", \"Coin-30cm-30cm\",\n",
    "               \"Eraser-10cm-10cm\", \"Eraser-10cm-30cm\", \"Eraser-30cm-10cm\", \"Eraser-30cm-30cm\",\n",
    "               \"Eraser-150cm-10cm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9413a427",
   "metadata": {},
   "source": [
    "## Modify ```# Parameters```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "adc_threshold = 1  # ADC threshold to start data collection\n",
    "num_items_to_collect = 100  # Number of ADC values to collect\n",
    "class_label = class_names.index(\"Coin-10cm-10cm\")  # Class label for the current data collection - ignored if is_demo is True\n",
    "filepath = \"data.csv\"  # Name of the CSV file to save the data - ignored if is_demo is True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880432a",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176329e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_demo and not is_training:\n",
    "    # Initialize serial communication\n",
    "    ser = serial.Serial(port=\"COM13\", baudrate=230400 , timeout=1)  # Configure the serial port\n",
    "    ser.flush()  # Flush the serial buffer to clear any existing data\n",
    "\n",
    "\n",
    "    # Function to read ADC value from the serial port\n",
    "    def read_adc_value():\n",
    "        \"\"\"\n",
    "        Reads a single ADC value from the serial port.\n",
    "        Returns:\n",
    "            int: The ADC value if successfully read, otherwise None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if ser.in_waiting > 0:  # Check if data is available in the serial buffer\n",
    "                line = ser.readline().strip()  # Read and decode the line\n",
    "                return int(line)  # Convert the line to an integer\n",
    "        except ValueError:\n",
    "            # Ignore invalid data that cannot be converted to an integer\n",
    "            pass\n",
    "        return None\n",
    "\n",
    "\n",
    "    # Function to save collected data to a CSV file\n",
    "    def save_to_csv(data, label, filepath):\n",
    "        \"\"\"\n",
    "        Saves the collected data to a CSV file.\n",
    "        Args:\n",
    "            data (list): The list of collected ADC values.\n",
    "            label (int): The class label for the data.\n",
    "            filepath (str): The name of the CSV file to save the data.\n",
    "        \"\"\"\n",
    "        # Add the new row to the dataframe\n",
    "        df.loc[len(df)] = [label] + data\n",
    "        # Save the dataframe to a CSV file\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "    # Read existing CSV file if it exists, otherwise create a new dataframe\n",
    "    if not is_demo:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)  # Try to read the existing CSV file\n",
    "        except FileNotFoundError:\n",
    "            # Create a new dataframe with appropriate columns if the file doesn't exist\n",
    "            df = pd.DataFrame(columns=[\"label\"] + [\"data\" + str(i + 1) for i in range(num_items_to_collect)])\n",
    "\n",
    "    # Data collection process\n",
    "    data = []  # List to store collected ADC values\n",
    "\n",
    "    try:\n",
    "        print(\"Data collection started...\")\n",
    "        activated = False  # Flag to indicate if the threshold has been crossed\n",
    "\n",
    "        # Collect data until the required number of items is reached\n",
    "        while len(data) < num_items_to_collect:\n",
    "            adc_value = read_adc_value()  # Read an ADC value\n",
    "            if adc_value is not None:\n",
    "                if not activated and adc_value > adc_threshold:\n",
    "                    # Start collecting data only after the threshold is crossed\n",
    "                    activated = True\n",
    "                    print(\"Threshold crossed. Collecting data...\")\n",
    "                if activated:\n",
    "                    # Append the ADC value to the data list\n",
    "                    data.append(adc_value)\n",
    "\n",
    "        print(\"Data collection completed.\")\n",
    "    except KeyboardInterrupt:\n",
    "        # Handle manual interruption (Ctrl+C)\n",
    "        print(\"Data collection interrupted.\")\n",
    "    except Exception as e:\n",
    "        # Handle any other exceptions\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Save the collected data to a CSV file\n",
    "    if not is_demo:\n",
    "        if data:\n",
    "            save_to_csv(data, class_label, filepath)  # Save the data with the class label\n",
    "            print(\"Data saved to CSV.\")\n",
    "        else:\n",
    "            print(\"No data collected.\")\n",
    "    else:\n",
    "        # For demonstration, just print the collected data\n",
    "        print(\"Collected data (for demo)\")\n",
    "\n",
    "    # Close the serial port\n",
    "    ser.close()\n",
    "\n",
    "# Dummy data for training only\n",
    "data = [0] * num_items_to_collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a21e5",
   "metadata": {},
   "source": [
    "## Plot the latest collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a5b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(data)), data)  # Plot the ADC values\n",
    "plt.xlabel(\"Sample Number\")  # Label for the x-axis\n",
    "plt.ylabel(\"ADC Value\")  # Label for the y-axis\n",
    "plt.title(\"ADC Values Over Time\")  # Title of the plot\n",
    "plt.grid()  # Show grid lines on the plot\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8865f5",
   "metadata": {},
   "source": [
    "# Fourier Transform ([Credit](https://www.youtube.com/watch?v=s2K1JfNR7Sc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1ef41",
   "metadata": {},
   "source": [
    "## Compute the Fast Fourier Transform (FFT) and plot the Power Spectral Density (PSD) for the latest collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1dde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 5530  # Sampling frequency in Hz\n",
    "\n",
    "n = len(data)  # Number of samples\n",
    "\n",
    "# Compute the FFT and Power Spectral Density (PSD)\n",
    "fhat = np.fft.fft(data, n)\n",
    "PSD = fhat * np.conj(fhat) / n\n",
    "freq = (1 / (n * (1 / fs))) * np.arange(n)  # Frequency bins\n",
    "L = np.arange(1, np.floor(n / 2), dtype='int')  # Positive frequencies\n",
    "\n",
    "# Plot the Power Spectral Density\n",
    "plt.plot(freq[L], PSD[L].real, linewidth=2)\n",
    "plt.xlabel('Power spectrum frequency (Hz)')\n",
    "plt.ylabel('Power (dB)')\n",
    "plt.title('Power Spectral Density')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597820c6",
   "metadata": {},
   "source": [
    "## Fourier transform entire dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psd(data):\n",
    "    \"\"\"\n",
    "    Computes the Power Spectral Density (PSD) of the input signal.\n",
    "    Args:\n",
    "        data (array-like): Input time-domain signal.\n",
    "    Returns:\n",
    "        array: PSD of the input signal.\n",
    "    \"\"\"\n",
    "    data = data - np.mean(data)  # Remove DC offset\n",
    "    n = len(data)\n",
    "    fhat = np.fft.fft(data, n)\n",
    "    PSD = (fhat * np.conj(fhat)) / n\n",
    "    return PSD[:n // 2].real  # Return only the positive frequencies\n",
    "\n",
    "if not is_demo or is_training:\n",
    "    df = pd.read_csv(filepath)  # Read the existing CSV file\n",
    "\n",
    "    # Create a new DataFrame to store PSD-transformed data\n",
    "    psd_length = len(df.columns) - 1  # Same number of samples per row (excluding label)\n",
    "    df_psd = pd.DataFrame(columns=[\"label\"] + [f\"psd{i+1}\" for i in range(psd_length // 2)])\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        label = int(df.iloc[i, 0])  # Extract and ensure label is int\n",
    "        data = df.iloc[i, 1:].values.astype(float)  # Extract signal values\n",
    "        psd = compute_psd(data)  # Compute PSD\n",
    "        df_psd.loc[i] = [label] + list(psd)  # Assign to new DataFrame\n",
    "\n",
    "    # Convert the label column to integers\n",
    "    df_psd[\"label\"] = df_psd[\"label\"].astype(int)\n",
    "\n",
    "    # Save the PSD-transformed data to a new CSV file\n",
    "    filepath_psd = filepath.replace(\"data\", \"data_psd\")\n",
    "    df_psd.to_csv(filepath_psd, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e9ceb",
   "metadata": {},
   "source": [
    "# Train Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60a266",
   "metadata": {},
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiezoDataset(Dataset):\n",
    "    def __init__(self, dataframe, mean, std):\n",
    "        self.X = dataframe.drop(columns=['label']).values.astype(np.float32)\n",
    "\n",
    "        # Normalize using train stats\n",
    "        epsilon = 1e-8  # prevent divide-by-zero\n",
    "        self.X = (self.X - mean) / (std + epsilon)\n",
    "        \n",
    "        self.y = dataframe['label'].values.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a999b9",
   "metadata": {},
   "source": [
    "## Create Dataset instance from the defined Class and then the DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbce88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data_psd.csv\")\n",
    "\n",
    "# Split the dataset into training (60%), validation (20%), and test (20%) sets\n",
    "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42, stratify=df['label'])\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify=df_temp['label'])\n",
    "\n",
    "# Drop the 'label' column to isolate the input features\n",
    "X_train = df_train.drop(columns=['label']).values\n",
    "\n",
    "# Compute mean and std from the training features\n",
    "means = np.mean(X_train, axis=0)\n",
    "devs = np.std(X_train, axis=0)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PiezoDataset(df_train, means, devs)\n",
    "val_dataset = PiezoDataset(df_val, means, devs)\n",
    "test_dataset = PiezoDataset(df_test, means, devs)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"Train dataset shape: {train_dataset.X.shape}\")\n",
    "print(f\"Validation dataset shape: {val_dataset.X.shape}\")\n",
    "print(f\"Test dataset shape: {test_dataset.X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d4c1df",
   "metadata": {},
   "source": [
    "## Design the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PiezoNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, device='cpu'):\n",
    "        super(PiezoNN, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc1 = nn.Linear(input_size, 512)  # Increase units in the first layer\n",
    "        self.fc2 = nn.Linear(512, 256)         # Decrease units in the second layer\n",
    "        self.fc3 = nn.Linear(256, num_classes) # Output layer\n",
    "        self.bn1 = nn.BatchNorm1d(512)         # Batch normalization after first layer\n",
    "        self.bn2 = nn.BatchNorm1d(256)         # Batch normalization after second layer\n",
    "        self.dropout = nn.Dropout()         # Dropout to prevent overfitting\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # ReLU activation after the first layer\n",
    "        x = self.bn1(x)          # Batch Normalization\n",
    "        x = self.dropout(x)      # Apply dropout for regularization\n",
    "        x = F.relu(self.fc2(x))  # ReLU activation after the second layer\n",
    "        x = self.bn2(x)          # Batch Normalization\n",
    "        x = self.dropout(x)      # Apply dropout for regularization\n",
    "        x = self.fc3(x)          # Output layer\n",
    "        return x                 # Return without softmax (F.cross_entropy will do it automatically)\n",
    "\n",
    "    def train_model(self, epochs, optimizer, scheduler, train_loader, val_loader, verbose=True):\n",
    "        self.loss_train_log = []\n",
    "        self.loss_val_log = []\n",
    "        self.best_loss = np.inf\n",
    "\n",
    "        # Move model to device\n",
    "        self.to(device=self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            for x, y in train_loader:\n",
    "                x = x.to(device=self.device)\n",
    "                y = y.to(device=self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                yhat = self.forward(x)\n",
    "                loss = F.cross_entropy(yhat, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Calculate loss for training and validation sets\n",
    "            loss_train = self.evaluate(train_loader)\n",
    "            self.loss_train_log.append(loss_train)\n",
    "\n",
    "            loss_val = self.evaluate(val_loader)\n",
    "            self.loss_val_log.append(loss_val)\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step(loss_val)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch+1}/{epochs}: Train Loss = {loss_train:.4f}, Val Loss = {loss_val:.4f}')\n",
    "\n",
    "            if loss_val < self.best_loss:\n",
    "                self.best_loss = loss_val\n",
    "                best_epoch = epoch + 1\n",
    "                torch.save(self.state_dict(), \"best_model.pt\") # Save the best model\n",
    "                \n",
    "        print(f'Best model saved at epoch {best_epoch} with loss {self.best_loss:.4f}.')\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.eval()\n",
    "        loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x = x.to(device=self.device)\n",
    "                y = y.to(device=self.device)\n",
    "                yhat = self.forward(x)\n",
    "                loss += F.cross_entropy(yhat, y, reduction='sum')\n",
    "        \n",
    "        loss /= len(loader.dataset)\n",
    "        return loss.cpu()\n",
    "\n",
    "    def predict(self, loader):\n",
    "        self.eval()\n",
    "        x_all, y_all, logit = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x_all += [x]\n",
    "                y_all += [y]\n",
    "                x = x.to(device=self.device)  # Move data to the same device as model\n",
    "                y = y.to(device=self.device)  # Move labels to the same device as model\n",
    "                yhat = self.forward(x)\n",
    "                logit.append(yhat.cpu())  # Move logits back to CPU for easier manipulation\n",
    "                \n",
    "            x_all, y_all, logit = torch.cat(x_all, dim=0), torch.cat(y_all, dim=0), torch.cat(logit, dim=0)\n",
    "            return x_all, y_all, logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b78a824",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b22c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_training:\n",
    "    # Model training\n",
    "    input_size = len(train_dataset[0][0])  # Number of features in your dataset\n",
    "    num_classes = len(class_names)  # Number of unique labels\n",
    "\n",
    "    epochs = 100\n",
    "\n",
    "    # Define possible hyperparameter values to test\n",
    "    lr_values = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    weight_decay_values = [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05]\n",
    "    patience_values = [3, 5, 7, 10, 15, 20]\n",
    "    factor_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "    # Initialize variables to store the best result\n",
    "    best_val_loss = float('inf')\n",
    "    best_hyperparameters = None\n",
    "    best_model_state = None\n",
    "    best_model_losses = {\n",
    "        \"loss_train_log\": [],\n",
    "        \"loss_val_log\": []\n",
    "    }\n",
    "\n",
    "    # Hyperparameter sweep\n",
    "    for lr in lr_values:\n",
    "        for weight_decay in weight_decay_values:\n",
    "            for patience in patience_values:\n",
    "                for factor in factor_values:\n",
    "                    print(f\"Testing with lr={lr}, weight_decay={weight_decay}, patience={patience}, factor={factor}\")\n",
    "\n",
    "                    # Re-initialize the model to start fresh for each hyperparameter combination\n",
    "                    model = PiezoNN(input_size=input_size, num_classes=num_classes, device=device)\n",
    "                    \n",
    "                    # Set up the optimizer and learning rate scheduler\n",
    "                    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience)\n",
    "                    \n",
    "                    # Train the model\n",
    "                    model.train_model(epochs=epochs, optimizer=optimizer, scheduler=scheduler, train_loader=train_loader, val_loader=val_loader)\n",
    "                    \n",
    "                    # Evaluate on the validation set\n",
    "                    val_loss = model.best_loss\n",
    "                    \n",
    "                    # Check if this is the best validation loss\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_hyperparameters = {'lr': lr, 'weight_decay': weight_decay, 'patience': patience, 'factor': factor}\n",
    "                        best_model_state = model.state_dict()  # Save the model state for best performance\n",
    "                        best_model_losses = {\n",
    "                            \"loss_train_log\": model.loss_train_log,\n",
    "                            \"loss_val_log\": model.loss_val_log\n",
    "                        }\n",
    "\n",
    "    # Save the best model\n",
    "    torch.save(best_model_state, 'best_model.pt')\n",
    "\n",
    "    # Print the best hyperparameters\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b712f",
   "metadata": {},
   "source": [
    "## Inspecting the training and validation loss for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b7de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"best_model_losses\" in locals():\n",
    "    # Plotting training vs val loss\n",
    "    plt.plot(best_model_losses[\"loss_train_log\"], label='Train Loss')\n",
    "    plt.plot(best_model_losses[\"loss_val_log\"], label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8723e",
   "metadata": {},
   "source": [
    "# Evaluate Classification Model (using test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0598a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_params = torch.load('best_model.pt')\n",
    "model_best = PiezoNN(input_size=len(train_dataset[0][0]), num_classes=len(class_names), device=device)\n",
    "model_best.load_state_dict(best_params)\n",
    "model_best.to(device)\n",
    "\n",
    "# Evaluate of test data\n",
    "x_all, y_all, logit = model_best.predict(test_loader)\n",
    "predictions = F.softmax(logit, dim=1).argmax(dim=1)\n",
    "\n",
    "# Calculate confusion matrix and other metrics\n",
    "y_true = y_all.cpu().numpy()  # True labels\n",
    "y_pred = predictions.cpu().numpy()  # Predicted labels\n",
    "cMat = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "precision = metrics.precision_score(y_true, y_pred, average='macro')  # Macro for multi-class\n",
    "recall = metrics.recall_score(y_true, y_pred, average='macro')  # Macro for multi-class\n",
    "f1 = metrics.f1_score(y_true, y_pred, average='macro')  # Macro for multi-class\n",
    "\n",
    "# Plot using Seaborn with index labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cMat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "\n",
    "metrics_text = f\"\"\"\n",
    "Accuracy       = {accuracy:.2f}\n",
    "Precision      = {precision:.2f}\n",
    "Recall         = {recall:.2f}\n",
    "F1 Score       = {f1:.2f}\n",
    "\"\"\"\n",
    "\n",
    "# Create a text box\n",
    "plt.gcf().text(0.5, -0.11, metrics_text.strip(), ha='center', fontsize=11, family='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa8053",
   "metadata": {},
   "source": [
    "# Demo Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc605148",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_demo:    \n",
    "    # Create a DataFrame for the demo data with a dummy label (0)\n",
    "    data_psd = compute_psd(data)\n",
    "    demo_df_psd = pd.DataFrame(columns=[\"label\"] + [f\"psd{i+1}\" for i in range(len(data_psd))])\n",
    "    demo_df_psd.loc[0] = [0] + list(data_psd)  # Assign to new DataFrame\n",
    "\n",
    "    # Create a PiezoDataset instance for the demo data\n",
    "    demo_data = PiezoDataset(demo_df_psd, means, devs)\n",
    "    demo_loader = DataLoader(demo_data, batch_size=1, shuffle=False)  # Create a DataLoader for the demo data\n",
    "\n",
    "    # Predict the class for the demo data\n",
    "    x, y, logit = model_best.predict(demo_loader)\n",
    "    ypred_demo = F.softmax(logit, dim=1).argmax(dim=1)  # Get the predicted class for the demo data\n",
    "    print(f\"Predicted class: {class_names[ypred_demo]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
